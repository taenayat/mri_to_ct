path=conda4ganlate/lib/python3.9/site-packages/ganslate

1.
path/utils/csv_saver.py - line 8
from: "self.df = self.df.append(row, ignore_index=True)"
to: "self.df = pd.concat([self.df, pd.DataFrame([row], columns=row.keys())], ignore_index=True)"

2.
path/utils/trackers/trainig.py - line 89 save_learning_curves
to:
"
        losses_detached = {}
        loss_names = list(losses[0].keys())
        if 'idt_A' in loss_names:
            loss_names.remove('idt_A')
        if 'idt_B' in loss_names:
            loss_names.remove('idt_B')

        for loss in loss_names:
            losses_detached[loss] = []

        for d in losses:
            for loss in loss_names:
                losses_detached[loss].append(d[loss].detach().cpu().item())

        losses_df = pd.DataFrame(losses_detached)
        losses_df.to_csv(Path(self.output_dir)/'training_losses.csv', index=False)
"

3.
path/utils/trackers/training.py - line 83 (self.tensorboard.log_iter)
from: "visuals=visuals"
to: "visuals=None"

4.
path/utils/trackers/tensorboard.py - line 17 (def log_iter)
from "..., visuals, mode, ..."
to "..., mode, visuals=None, ..."

5.
path/utils/trackers/tensorboard.py - line 37 (Normal images)
from:
"
        normal_visuals = process_visuals_wandb_tensorboard(visuals, image_window=None)
        self._log_images(iter_idx, normal_visuals, tag=mode)
"
to:
"
            normal_visuals = process_visuals_wandb_tensorboard(visuals, image_window=None)
            self._log_images(iter_idx, normal_visuals, tag=mode)
"

6.
mri_to_ct/dataset/val_test_dataset.py - line about 66 (in __getitem__)
from: 
"
        CT_tensor = min_max_normalize(CT_tensor, CT_tensor.min(), CT_tensor.max())
        MRI_tensor = min_max_normalize(MRI_tensor, MRI_tensor.min(), MRI_tensor.max())
"
to:
"
        self.MRI_min_value, self.MRI_max_value = MRI_tensor.min(), MRI_tensor.max()
        self.CT_min_value, self.CT_max_value = -1024, 3000

        CT_tensor = min_max_normalize(CT_tensor, self.CT_min_value, self.CT_max_value)
        MRI_tensor = min_max_normalize(MRI_tensor, self.MRI_min_value, self.MRI_max_value)
"

6.
mri_to_ct/dataset/val_test_dataset.py - line 87 (after __getitem__)
from: ""
to:
"
    def denormalize(self, tensor):
        return min_max_denormalize(tensor, self.CT_min_value, self.CT_max_value)
"

7.
path/utils/trackers/tensorboard.py - line 39
from: ""
to:
"
from torch.cuda import is_available, memory_allocated, memory_reserved

        if is_available():
            gpu_memory_reserved = memory_reserved()
            gpu_memory_allocated = memory_allocated()
            self.writer.add_scalar("GPU_RAM/Reserved", gpu_memory_reserved, iter_idx)
            self.writer.add_scalar("GPU_RAM/Allocated", gpu_memory_allocated, iter_idx)
"

8.
path/data/utils/ops.py
from:
"
import numpy as np


def pad(volume, target_shape):
    assert len(target_shape) == len(volume.shape)
    # By default no padding
    pad_width = [(0, 0) for _ in range(len(target_shape))]

    for dim in range(len(target_shape)):
        if target_shape[dim] > volume.shape[dim]:
            pad_total = target_shape[dim] - volume.shape[dim]
            pad_per_side = pad_total // 2
            pad_width[dim] = (pad_per_side, pad_total % 2 + pad_per_side)

    return np.pad(volume, pad_width, 'constant', constant_values=volume.min())
"
to:
"
import torch

def pad(volume, target_shape):
    assert len(target_shape) == len(volume.shape)
    
    # Default no padding
    pad_width = []
    
    for dim in range(len(target_shape) - 1, -1, -1):  # PyTorch's pad expects reversed order
        if target_shape[dim] > volume.shape[dim]:
            pad_total = target_shape[dim] - volume.shape[dim]
            pad_per_side = pad_total // 2
            pad_width.extend([pad_per_side, pad_total % 2 + pad_per_side])
        else:
            pad_width.extend([0, 0])
    # print(pad_width)
    padded_volume = torch.nn.functional.pad(
        volume, pad_width, mode='constant', value=volume.min().item()
    )
    return padded_volume
"

9.
mri_to_ct/dataset/val_test_dataset.py - new __getitem__
to:
"
    def __getitem__(self, index):
        final_index = index % self.num_datapoints
        
        ct_sample=self.ct_path[final_index]
        mri_sample=self.mri_path[final_index]
        mask_sample=self.mask_path[final_index]


        CT_image = sitk_utils.load(ct_sample)
        MRI_image = sitk_utils.load(mri_sample)
        mask = sitk_utils.load(mask_sample)

        CT_tensor = sitk_utils.get_tensor(CT_image)
        MRI_tensor = sitk_utils.get_tensor(MRI_image)
        mask_tensor = sitk_utils.get_tensor(mask)

        self.CT_min_value, self.CT_max_value = -1024, 3000
        CT_tensor = min_max_normalize(CT_tensor, self.CT_min_value, self.CT_max_value)
        MRI_tensor = z_score_squeeze(MRI_tensor)

        # print('before pad', MRI_tensor.shape, CT_tensor.shape)
        CT_tensor = pad(CT_tensor, (262,284,280))
        MRI_tensor = pad(MRI_tensor, (262,284,280))
        mask_tensor = pad(mask_tensor, (262,284,280))
        # print('after pad', MRI_tensor.shape, CT_tensor.shape)

        CT_tensor = CT_tensor.unsqueeze(0)
        MRI_tensor = MRI_tensor.unsqueeze(0)
        mask_tensor = mask_tensor.unsqueeze(0)

        mask_dict={"clean_mask": mask_tensor}
        return {'A': MRI_tensor, 'B': CT_tensor, "masks": mask_dict,"metadata": {"mri_path": str(mri_sample)}}
"


10.
path/engines/validator_tester.py line 28 (create function)
from: ""
to:
"
    ###################
    # def create_heatmap(self, real_B, fake_B):
    #     fill_image = torch.zeros_like(real_B, device=real_B.device)
    #     comp_image = torch.cat((fake_B, fill_image, real_B), dim=1)
    #     return comp_image
    def create_heatmap(self, real_B, fake_B):
        difference = (fake_B - real_B) / 2.0
        red = torch.clamp(difference, 0, 1)
        blue = torch.clamp(-difference, 0, 1)
        green = 1.0 - torch.abs(difference)
        composition_image = torch.cat((red,green,blue), dim=1)
        return composition_image
    ####################
"

11.
path/engines/validator_tester.py - end of _calculate_metrics function
from: ""
to:
"
        if "real_B" in self.visuals and "fake_B" in self.visuals:
            heatmap = self.create_heatmap(self.visuals["real_B"], self.visuals["fake_B"])
            self.visuals["clean_mask"] = heatmap
"


12.
path/engines/validator_tester.py line 28 (update create_heatmap function)
from:
"
    ###################
    def create_heatmap(self, real_B, fake_B):
        difference = (fake_B - real_B) / 2.0
        red = torch.clamp(difference, 0, 1)
        blue = torch.clamp(-difference, 0, 1)
        green = 1.0 - torch.abs(difference)
        composition_image = torch.cat((red,green,blue), dim=1)
        return composition_image
    ####################
"
to:
"
def create_heatmap(self, real_B, fake_B):
        difference = (fake_B - real_B) / 2.0

        # Create masks
        mask_red = difference > 0
        mask_blue = difference < 0
        mask_white = difference == 0

        # Initialize color channels
        red = torch.zeros_like(difference)
        green = torch.zeros_like(difference)
        blue = torch.zeros_like(difference)

        # Apply colors
        red[mask_red] = 1.0      # Red where fake_B > real_B
        blue[mask_blue] = 1.0    # Blue where fake_B < real_B
        red[mask_white] = 1.0    # White where fake_B == real_B
        green[mask_white] = 1.0
        blue[mask_white] = 1.0

        # Stack channels
        composition_image = torch.cat((red, green, blue), dim=1)

        return composition_image
"


13.
path/data/utils/ops.py - (create dynamic_pad_collate function)
from:""
to:
"

def dynamic_pad_collate(batch):
    """
    Pads 4D tensors in the batch to the size of the largest tensor in each spatial dimension.
    Uses the custom `pad` function which takes an image tensor and the desired shape.
    Handles dictionaries with keys 'A', 'B', and optionally 'masks' and 'metadata'.
    Assumes tensors have shape [C, D, H, W].
    """
    # Initialize lists for each key
    A_batch = [item['A'] for item in batch]

    # Determine max dimensions for 'A'
    max_depth = max(item.shape[1] for item in A_batch)
    max_height = max(item.shape[2] for item in A_batch)
    max_width = max(item.shape[3] for item in A_batch)
    desired_shape = (max_depth, max_height, max_width)

    padded_A = []
    padded_B = []
    masks = []
    metadata = []

    for item in batch:
        # if 'masks' in item:
        #     print('before',item['A'].shape, item['B'].shape, item['masks']['clean_mask'].shape)
        # else:
        #     print('before',item['A'].shape, item['B'].shape)

        # Pad 'A'
        padded_A_item = pad(item['A'].squeeze(0), desired_shape)
        padded_A.append(padded_A_item.unsqueeze(0))

        # Pad 'B'
        padded_B_item = pad(item['B'].squeeze(0), desired_shape)
        padded_B.append(padded_B_item.unsqueeze(0))

        # Pad 'masks' if present
        # if 'masks' in item:
        #     padded_mask = pad(item['masks']['clean_mask'], desired_shape)
        #     masks.append(padded_mask.unsqueeze(0))

        if 'masks' in item:
            mask_batch = {}
            for key, mask in item['masks'].items():
                padded_mask = pad(mask.squeeze(0), desired_shape)
                mask_batch[key] = padded_mask.unsqueeze(0)
            masks.append(mask_batch)
        # if 'masks' in item and item['masks'] is not None:
        #     mask_batch = {}
        #     for key, mask in item['masks'].items():
        #         padded_mask = pad(mask.squeeze(0), desired_shape)
        #         mask_batch[key] = padded_mask.unsqueeze(0)
        #     masks.append(mask_batch)
        # else:
        #     masks.append({})

        # Collect metadata if present
        if 'metadata' in item:
            metadata.append(item['metadata'])
        # if 'metadata' in item and item['metadata'] is not None:
        #     metadata.append(item['metadata'])
        # else:
        #     metadata.append({})

    # for a,b in zip(padded_A,padded_B):
    #     print('after',a.shape, b.shape)

    batch_dict = {
        'A': torch.stack(padded_A),
        'B': torch.stack(padded_B)
    }

    # if masks:
    #     batch_dict['masks'] = masks
    # if metadata:
    #     batch_dict['metadata'] = metadata
    if masks:
        collated_masks = {}
        for mask_batch in masks:
            for key, mask in mask_batch.items():
                if key not in collated_masks:
                    collated_masks[key] = []
                collated_masks[key].append(mask)
        # Stack masks for each key
        for key in collated_masks:
            collated_masks[key] = torch.stack(collated_masks[key])
        batch_dict['masks'] = collated_masks

    # Collate 'metadata' into a dictionary of lists
    if metadata:
        collated_metadata = {}
        for meta in metadata:
            for key, value in meta.items():
                if key not in collated_metadata:
                    collated_metadata[key] = []
                collated_metadata[key].append(value)
        batch_dict['metadata'] = collated_metadata

    return batch_dict
"

14.
path/utils/builders.py - line 73-78 (update return)
from:
"
    return DataLoader(dataset,
                      sampler=sampler,
                      batch_size=conf[conf.mode].batch_size,
                      num_workers=conf[conf.mode].dataset.num_workers,
                      pin_memory=conf[conf.mode].dataset.pin_memory)
"
to:
"
    return DataLoader(dataset,
                      sampler=sampler,
                      batch_size=conf[conf.mode].batch_size,
                      num_workers=conf[conf.mode].dataset.num_workers,
                      pin_memory=conf[conf.mode].dataset.pin_memory,
                      collate_fn=dynamic_pad_collate) # added line
"

15.
path/data/utils/ops.py - (update dynamic_pad_collate function)
to:
"
def dynamic_pad_collate(batch):
    # Initialize lists for each key
    A_batch = [item['A'] for item in batch]

    # Determine max dimensions for 'A'
    max_depth = max(item.shape[1] for item in A_batch)
    max_height = max(item.shape[2] for item in A_batch)
    max_width = max(item.shape[3] for item in A_batch)
    desired_shape = (max_depth, max_height, max_width)

    padded_A = []
    padded_B = []
    masks = []
    metadata = []

    for item in batch:
        padded_A_item = pad(item['A'].squeeze(0), desired_shape)
        padded_A.append(padded_A_item.unsqueeze(0))

        if 'B' in item:
            padded_B_item = pad(item['B'].squeeze(0), desired_shape)
            padded_B.append(padded_B_item.unsqueeze(0))

        if 'masks' in item:
            mask_batch = {}
            for key, mask in item['masks'].items():
                padded_mask = pad(mask.squeeze(0), desired_shape)
                mask_batch[key] = padded_mask.unsqueeze(0)
            masks.append(mask_batch)

        if 'metadata' in item:
            metadata.append(item['metadata'])

    batch_dict = {
        'A': torch.stack(padded_A)
        # 'B': torch.stack(padded_B)
    }
    if padded_B:
        batch_dict['B'] = torch.stack(padded_B)

    if masks:
        collated_masks = {}
        for mask_batch in masks:
            for key, mask in mask_batch.items():
                if key not in collated_masks:
                    collated_masks[key] = []
                collated_masks[key].append(mask)
        for key in collated_masks:
            collated_masks[key] = torch.stack(collated_masks[key])
        batch_dict['masks'] = collated_masks

    if metadata:
        collated_metadata = {}
        for meta in metadata:
            for key, value in meta.items():
                if key not in collated_metadata:
                    collated_metadata[key] = []
                collated_metadata[key].append(value)
        batch_dict['metadata'] = collated_metadata

    return batch_dict
"

16.
path/data/utils/ops.py - (added remove_artifact function; called in train & valtest dataset classes)
to:
"
def remove_artifacts(image_sitk, threshold=-150):
    binary_mask = sitk.BinaryThreshold(image_sitk, lowerThreshold=threshold, upperThreshold=3000)
    connected_components = sitk.ConnectedComponent(binary_mask)
    labeled_components = sitk.RelabelComponent(connected_components, sortByObjectSize=True)
    skull_mask = sitk.BinaryThreshold(labeled_components, lowerThreshold=1, upperThreshold=1, insideValue=1, outsideValue=0)
    skull_image = sitk.Mask(image_sitk, skull_mask)
    return skull_image
"
